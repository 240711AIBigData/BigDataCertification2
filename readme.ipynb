{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ArrowDtype', 'BooleanDtype', 'Categorical', 'CategoricalDtype', 'CategoricalIndex', 'DataFrame', 'DateOffset', 'DatetimeIndex', 'DatetimeTZDtype', 'ExcelFile', 'ExcelWriter', 'Flags', 'Float32Dtype', 'Float64Dtype', 'Grouper', 'HDFStore', 'Index', 'IndexSlice', 'Int16Dtype', 'Int32Dtype', 'Int64Dtype', 'Int8Dtype', 'Interval', 'IntervalDtype', 'IntervalIndex', 'MultiIndex', 'NA', 'NaT', 'NamedAgg', 'Period', 'PeriodDtype', 'PeriodIndex', 'RangeIndex', 'Series', 'SparseDtype', 'StringDtype', 'Timedelta', 'TimedeltaIndex', 'Timestamp', 'UInt16Dtype', 'UInt32Dtype', 'UInt64Dtype', 'UInt8Dtype', '__all__', '__builtins__', '__cached__', '__doc__', '__docformat__', '__file__', '__git_version__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_built_with_meson', '_config', '_is_numpy_dev', '_libs', '_pandas_datetime_CAPI', '_pandas_parser_CAPI', '_testing', '_typing', '_version_meson', 'annotations', 'api', 'array', 'arrays', 'bdate_range', 'compat', 'concat', 'core', 'crosstab', 'cut', 'date_range', 'describe_option', 'errors', 'eval', 'factorize', 'from_dummies', 'get_dummies', 'get_option', 'infer_freq', 'interval_range', 'io', 'isna', 'isnull', 'json_normalize', 'lreshape', 'melt', 'merge', 'merge_asof', 'merge_ordered', 'notna', 'notnull', 'offsets', 'option_context', 'options', 'pandas', 'period_range', 'pivot', 'pivot_table', 'plotting', 'qcut', 'read_clipboard', 'read_csv', 'read_excel', 'read_feather', 'read_fwf', 'read_gbq', 'read_hdf', 'read_html', 'read_json', 'read_orc', 'read_parquet', 'read_pickle', 'read_sas', 'read_spss', 'read_sql', 'read_sql_query', 'read_sql_table', 'read_stata', 'read_table', 'read_xml', 'reset_option', 'set_eng_float_format', 'set_option', 'show_versions', 'test', 'testing', 'timedelta_range', 'to_datetime', 'to_numeric', 'to_pickle', 'to_timedelta', 'tseries', 'unique', 'util', 'value_counts', 'wide_to_long']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_datetime in module pandas.core.tools.datetimes:\n",
      "\n",
      "to_datetime(arg: 'DatetimeScalarOrArrayConvertible | DictConvertible', errors: 'DateTimeErrorChoices' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool' = False, format: 'str | None' = None, exact: 'bool | lib.NoDefault' = <no_default>, unit: 'str | None' = None, infer_datetime_format: 'lib.NoDefault | bool' = <no_default>, origin: 'str' = 'unix', cache: 'bool' = True) -> 'DatetimeIndex | Series | DatetimeScalar | NaTType | None'\n",
      "    Convert argument to datetime.\n",
      "\n",
      "    This function converts a scalar, array-like, :class:`Series` or\n",
      "    :class:`DataFrame`/dict-like to a pandas datetime object.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    arg : int, float, str, datetime, list, tuple, 1-d array, Series, DataFrame/dict-like\n",
      "        The object to convert to a datetime. If a :class:`DataFrame` is provided, the\n",
      "        method expects minimally the following columns: :const:`\"year\"`,\n",
      "        :const:`\"month\"`, :const:`\"day\"`. The column \"year\"\n",
      "        must be specified in 4-digit format.\n",
      "    errors : {'ignore', 'raise', 'coerce'}, default 'raise'\n",
      "        - If :const:`'raise'`, then invalid parsing will raise an exception.\n",
      "        - If :const:`'coerce'`, then invalid parsing will be set as :const:`NaT`.\n",
      "        - If :const:`'ignore'`, then invalid parsing will return the input.\n",
      "    dayfirst : bool, default False\n",
      "        Specify a date parse order if `arg` is str or is list-like.\n",
      "        If :const:`True`, parses dates with the day first, e.g. :const:`\"10/11/12\"`\n",
      "        is parsed as :const:`2012-11-10`.\n",
      "\n",
      "        .. warning::\n",
      "\n",
      "            ``dayfirst=True`` is not strict, but will prefer to parse\n",
      "            with day first.\n",
      "\n",
      "    yearfirst : bool, default False\n",
      "        Specify a date parse order if `arg` is str or is list-like.\n",
      "\n",
      "        - If :const:`True` parses dates with the year first, e.g.\n",
      "          :const:`\"10/11/12\"` is parsed as :const:`2010-11-12`.\n",
      "        - If both `dayfirst` and `yearfirst` are :const:`True`, `yearfirst` is\n",
      "          preceded (same as :mod:`dateutil`).\n",
      "\n",
      "        .. warning::\n",
      "\n",
      "            ``yearfirst=True`` is not strict, but will prefer to parse\n",
      "            with year first.\n",
      "\n",
      "    utc : bool, default False\n",
      "        Control timezone-related parsing, localization and conversion.\n",
      "\n",
      "        - If :const:`True`, the function *always* returns a timezone-aware\n",
      "          UTC-localized :class:`Timestamp`, :class:`Series` or\n",
      "          :class:`DatetimeIndex`. To do this, timezone-naive inputs are\n",
      "          *localized* as UTC, while timezone-aware inputs are *converted* to UTC.\n",
      "\n",
      "        - If :const:`False` (default), inputs will not be coerced to UTC.\n",
      "          Timezone-naive inputs will remain naive, while timezone-aware ones\n",
      "          will keep their time offsets. Limitations exist for mixed\n",
      "          offsets (typically, daylight savings), see :ref:`Examples\n",
      "          <to_datetime_tz_examples>` section for details.\n",
      "\n",
      "        .. warning::\n",
      "\n",
      "            In a future version of pandas, parsing datetimes with mixed time\n",
      "            zones will raise an error unless `utc=True`.\n",
      "            Please specify `utc=True` to opt in to the new behaviour\n",
      "            and silence this warning. To create a `Series` with mixed offsets and\n",
      "            `object` dtype, please use `apply` and `datetime.datetime.strptime`.\n",
      "\n",
      "        See also: pandas general documentation about `timezone conversion and\n",
      "        localization\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n",
      "        #time-zone-handling>`_.\n",
      "\n",
      "    format : str, default None\n",
      "        The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n",
      "        `strftime documentation\n",
      "        <https://docs.python.org/3/library/datetime.html\n",
      "        #strftime-and-strptime-behavior>`_ for more information on choices, though\n",
      "        note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n",
      "        You can also pass:\n",
      "\n",
      "        - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n",
      "          time string (not necessarily in exactly the same format);\n",
      "        - \"mixed\", to infer the format for each element individually. This is risky,\n",
      "          and you should probably use it along with `dayfirst`.\n",
      "\n",
      "        .. note::\n",
      "\n",
      "            If a :class:`DataFrame` is passed, then `format` has no effect.\n",
      "\n",
      "    exact : bool, default True\n",
      "        Control how `format` is used:\n",
      "\n",
      "        - If :const:`True`, require an exact `format` match.\n",
      "        - If :const:`False`, allow the `format` to match anywhere in the target\n",
      "          string.\n",
      "\n",
      "        Cannot be used alongside ``format='ISO8601'`` or ``format='mixed'``.\n",
      "    unit : str, default 'ns'\n",
      "        The unit of the arg (D,s,ms,us,ns) denote the unit, which is an\n",
      "        integer or float number. This will be based off the origin.\n",
      "        Example, with ``unit='ms'`` and ``origin='unix'``, this would calculate\n",
      "        the number of milliseconds to the unix epoch start.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If :const:`True` and no `format` is given, attempt to infer the format\n",
      "        of the datetime strings based on the first non-NaN element,\n",
      "        and if it can be inferred, switch to a faster method of parsing them.\n",
      "        In some cases this can increase the parsing speed by ~5-10x.\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "            A strict version of this argument is now the default, passing it has\n",
      "            no effect.\n",
      "\n",
      "    origin : scalar, default 'unix'\n",
      "        Define the reference date. The numeric values would be parsed as number\n",
      "        of units (defined by `unit`) since this reference date.\n",
      "\n",
      "        - If :const:`'unix'` (or POSIX) time; origin is set to 1970-01-01.\n",
      "        - If :const:`'julian'`, unit must be :const:`'D'`, and origin is set to\n",
      "          beginning of Julian Calendar. Julian day number :const:`0` is assigned\n",
      "          to the day starting at noon on January 1, 4713 BC.\n",
      "        - If Timestamp convertible (Timestamp, dt.datetime, np.datetimt64 or date\n",
      "          string), origin is set to Timestamp identified by origin.\n",
      "        - If a float or integer, origin is the difference\n",
      "          (in units determined by the ``unit`` argument) relative to 1970-01-01.\n",
      "    cache : bool, default True\n",
      "        If :const:`True`, use a cache of unique, converted dates to apply the\n",
      "        datetime conversion. May produce significant speed-up when parsing\n",
      "        duplicate date strings, especially ones with timezone offsets. The cache\n",
      "        is only used when there are at least 50 values. The presence of\n",
      "        out-of-bounds values will render the cache unusable and may slow down\n",
      "        parsing.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    datetime\n",
      "        If parsing succeeded.\n",
      "        Return type depends on input (types in parenthesis correspond to\n",
      "        fallback in case of unsuccessful timezone or out-of-range timestamp\n",
      "        parsing):\n",
      "\n",
      "        - scalar: :class:`Timestamp` (or :class:`datetime.datetime`)\n",
      "        - array-like: :class:`DatetimeIndex` (or :class:`Series` with\n",
      "          :class:`object` dtype containing :class:`datetime.datetime`)\n",
      "        - Series: :class:`Series` of :class:`datetime64` dtype (or\n",
      "          :class:`Series` of :class:`object` dtype containing\n",
      "          :class:`datetime.datetime`)\n",
      "        - DataFrame: :class:`Series` of :class:`datetime64` dtype (or\n",
      "          :class:`Series` of :class:`object` dtype containing\n",
      "          :class:`datetime.datetime`)\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    ParserError\n",
      "        When parsing a date from string fails.\n",
      "    ValueError\n",
      "        When another datetime conversion error happens. For example when one\n",
      "        of 'year', 'month', day' columns is missing in a :class:`DataFrame`, or\n",
      "        when a Timezone-aware :class:`datetime.datetime` is found in an array-like\n",
      "        of mixed time offsets, and ``utc=False``.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.astype : Cast argument to a specified dtype.\n",
      "    to_timedelta : Convert argument to timedelta.\n",
      "    convert_dtypes : Convert dtypes.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "\n",
      "    Many input types are supported, and lead to different output types:\n",
      "\n",
      "    - **scalars** can be int, float, str, datetime object (from stdlib :mod:`datetime`\n",
      "      module or :mod:`numpy`). They are converted to :class:`Timestamp` when\n",
      "      possible, otherwise they are converted to :class:`datetime.datetime`.\n",
      "      None/NaN/null scalars are converted to :const:`NaT`.\n",
      "\n",
      "    - **array-like** can contain int, float, str, datetime objects. They are\n",
      "      converted to :class:`DatetimeIndex` when possible, otherwise they are\n",
      "      converted to :class:`Index` with :class:`object` dtype, containing\n",
      "      :class:`datetime.datetime`. None/NaN/null entries are converted to\n",
      "      :const:`NaT` in both cases.\n",
      "\n",
      "    - **Series** are converted to :class:`Series` with :class:`datetime64`\n",
      "      dtype when possible, otherwise they are converted to :class:`Series` with\n",
      "      :class:`object` dtype, containing :class:`datetime.datetime`. None/NaN/null\n",
      "      entries are converted to :const:`NaT` in both cases.\n",
      "\n",
      "    - **DataFrame/dict-like** are converted to :class:`Series` with\n",
      "      :class:`datetime64` dtype. For each row a datetime is created from assembling\n",
      "      the various dataframe columns. Column keys can be common abbreviations\n",
      "      like ['year', 'month', 'day', 'minute', 'second', 'ms', 'us', 'ns']) or\n",
      "      plurals of the same.\n",
      "\n",
      "    The following causes are responsible for :class:`datetime.datetime` objects\n",
      "    being returned (possibly inside an :class:`Index` or a :class:`Series` with\n",
      "    :class:`object` dtype) instead of a proper pandas designated type\n",
      "    (:class:`Timestamp`, :class:`DatetimeIndex` or :class:`Series`\n",
      "    with :class:`datetime64` dtype):\n",
      "\n",
      "    - when any input element is before :const:`Timestamp.min` or after\n",
      "      :const:`Timestamp.max`, see `timestamp limitations\n",
      "      <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n",
      "      #timeseries-timestamp-limits>`_.\n",
      "\n",
      "    - when ``utc=False`` (default) and the input is an array-like or\n",
      "      :class:`Series` containing mixed naive/aware datetime, or aware with mixed\n",
      "      time offsets. Note that this happens in the (quite frequent) situation when\n",
      "      the timezone has a daylight savings policy. In that case you may wish to\n",
      "      use ``utc=True``.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "\n",
      "    **Handling various input formats**\n",
      "\n",
      "    Assembling a datetime from multiple columns of a :class:`DataFrame`. The keys\n",
      "    can be common abbreviations like ['year', 'month', 'day', 'minute', 'second',\n",
      "    'ms', 'us', 'ns']) or plurals of the same\n",
      "\n",
      "    >>> df = pd.DataFrame({'year': [2015, 2016],\n",
      "    ...                    'month': [2, 3],\n",
      "    ...                    'day': [4, 5]})\n",
      "    >>> pd.to_datetime(df)\n",
      "    0   2015-02-04\n",
      "    1   2016-03-05\n",
      "    dtype: datetime64[ns]\n",
      "\n",
      "    Using a unix epoch time\n",
      "\n",
      "    >>> pd.to_datetime(1490195805, unit='s')\n",
      "    Timestamp('2017-03-22 15:16:45')\n",
      "    >>> pd.to_datetime(1490195805433502912, unit='ns')\n",
      "    Timestamp('2017-03-22 15:16:45.433502912')\n",
      "\n",
      "    .. warning:: For float arg, precision rounding might happen. To prevent\n",
      "        unexpected behavior use a fixed-width exact type.\n",
      "\n",
      "    Using a non-unix epoch origin\n",
      "\n",
      "    >>> pd.to_datetime([1, 2, 3], unit='D',\n",
      "    ...                origin=pd.Timestamp('1960-01-01'))\n",
      "    DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'],\n",
      "                  dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "    **Differences with strptime behavior**\n",
      "\n",
      "    :const:`\"%f\"` will parse all the way up to nanoseconds.\n",
      "\n",
      "    >>> pd.to_datetime('2018-10-26 12:00:00.0000000011',\n",
      "    ...                format='%Y-%m-%d %H:%M:%S.%f')\n",
      "    Timestamp('2018-10-26 12:00:00.000000001')\n",
      "\n",
      "    **Non-convertible date/times**\n",
      "\n",
      "    Passing ``errors='coerce'`` will force an out-of-bounds date to :const:`NaT`,\n",
      "    in addition to forcing non-dates (or non-parseable dates) to :const:`NaT`.\n",
      "\n",
      "    >>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce')\n",
      "    NaT\n",
      "\n",
      "    .. _to_datetime_tz_examples:\n",
      "\n",
      "    **Timezones and time offsets**\n",
      "\n",
      "    The default behaviour (``utc=False``) is as follows:\n",
      "\n",
      "    - Timezone-naive inputs are converted to timezone-naive :class:`DatetimeIndex`:\n",
      "\n",
      "    >>> pd.to_datetime(['2018-10-26 12:00:00', '2018-10-26 13:00:15'])\n",
      "    DatetimeIndex(['2018-10-26 12:00:00', '2018-10-26 13:00:15'],\n",
      "                  dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "    - Timezone-aware inputs *with constant time offset* are converted to\n",
      "      timezone-aware :class:`DatetimeIndex`:\n",
      "\n",
      "    >>> pd.to_datetime(['2018-10-26 12:00 -0500', '2018-10-26 13:00 -0500'])\n",
      "    DatetimeIndex(['2018-10-26 12:00:00-05:00', '2018-10-26 13:00:00-05:00'],\n",
      "                  dtype='datetime64[ns, UTC-05:00]', freq=None)\n",
      "\n",
      "    - However, timezone-aware inputs *with mixed time offsets* (for example\n",
      "      issued from a timezone with daylight savings, such as Europe/Paris)\n",
      "      are **not successfully converted** to a :class:`DatetimeIndex`.\n",
      "      Parsing datetimes with mixed time zones will show a warning unless\n",
      "      `utc=True`. If you specify `utc=False` the warning below will be shown\n",
      "      and a simple :class:`Index` containing :class:`datetime.datetime`\n",
      "      objects will be returned:\n",
      "\n",
      "    >>> pd.to_datetime(['2020-10-25 02:00 +0200',\n",
      "    ...                 '2020-10-25 04:00 +0100'])  # doctest: +SKIP\n",
      "    FutureWarning: In a future version of pandas, parsing datetimes with mixed\n",
      "    time zones will raise an error unless `utc=True`. Please specify `utc=True`\n",
      "    to opt in to the new behaviour and silence this warning. To create a `Series`\n",
      "    with mixed offsets and `object` dtype, please use `apply` and\n",
      "    `datetime.datetime.strptime`.\n",
      "    Index([2020-10-25 02:00:00+02:00, 2020-10-25 04:00:00+01:00],\n",
      "          dtype='object')\n",
      "\n",
      "    - A mix of timezone-aware and timezone-naive inputs is also converted to\n",
      "      a simple :class:`Index` containing :class:`datetime.datetime` objects:\n",
      "\n",
      "    >>> from datetime import datetime\n",
      "    >>> pd.to_datetime([\"2020-01-01 01:00:00-01:00\",\n",
      "    ...                 datetime(2020, 1, 1, 3, 0)])  # doctest: +SKIP\n",
      "    FutureWarning: In a future version of pandas, parsing datetimes with mixed\n",
      "    time zones will raise an error unless `utc=True`. Please specify `utc=True`\n",
      "    to opt in to the new behaviour and silence this warning. To create a `Series`\n",
      "    with mixed offsets and `object` dtype, please use `apply` and\n",
      "    `datetime.datetime.strptime`.\n",
      "    Index([2020-01-01 01:00:00-01:00, 2020-01-01 03:00:00], dtype='object')\n",
      "\n",
      "    |\n",
      "\n",
      "    Setting ``utc=True`` solves most of the above issues:\n",
      "\n",
      "    - Timezone-naive inputs are *localized* as UTC\n",
      "\n",
      "    >>> pd.to_datetime(['2018-10-26 12:00', '2018-10-26 13:00'], utc=True)\n",
      "    DatetimeIndex(['2018-10-26 12:00:00+00:00', '2018-10-26 13:00:00+00:00'],\n",
      "                  dtype='datetime64[ns, UTC]', freq=None)\n",
      "\n",
      "    - Timezone-aware inputs are *converted* to UTC (the output represents the\n",
      "      exact same datetime, but viewed from the UTC time offset `+00:00`).\n",
      "\n",
      "    >>> pd.to_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'],\n",
      "    ...                utc=True)\n",
      "    DatetimeIndex(['2018-10-26 17:30:00+00:00', '2018-10-26 17:00:00+00:00'],\n",
      "                  dtype='datetime64[ns, UTC]', freq=None)\n",
      "\n",
      "    - Inputs can contain both string or datetime, the above\n",
      "      rules still apply\n",
      "\n",
      "    >>> pd.to_datetime(['2018-10-26 12:00', datetime(2020, 1, 1, 18)], utc=True)\n",
      "    DatetimeIndex(['2018-10-26 12:00:00+00:00', '2020-01-01 18:00:00+00:00'],\n",
      "                  dtype='datetime64[ns, UTC]', freq=None)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(pd.to_datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calibration', 'cluster', 'covariance', 'cross_decomposition', 'datasets', 'decomposition', 'dummy', 'ensemble', 'exceptions', 'experimental', 'externals', 'feature_extraction', 'feature_selection', 'gaussian_process', 'inspection', 'isotonic', 'kernel_approximation', 'kernel_ridge', 'linear_model', 'manifold', 'metrics', 'mixture', 'model_selection', 'multiclass', 'multioutput', 'naive_bayes', 'neighbors', 'neural_network', 'pipeline', 'preprocessing', 'random_projection', 'semi_supervised', 'svm', 'tree', 'discriminant_analysis', 'impute', 'compose', 'clone', 'get_config', 'set_config', 'config_context', 'show_versions']\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "print(sklearn.__all__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Binarizer', 'FunctionTransformer', 'KBinsDiscretizer', 'KernelCenterer', 'LabelBinarizer', 'LabelEncoder', 'MaxAbsScaler', 'MinMaxScaler', 'MultiLabelBinarizer', 'Normalizer', 'OneHotEncoder', 'OrdinalEncoder', 'PolynomialFeatures', 'PowerTransformer', 'QuantileTransformer', 'RobustScaler', 'SplineTransformer', 'StandardScaler', 'TargetEncoder', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_csr_polynomial_expansion', '_data', '_discretization', '_encoders', '_function_transformer', '_label', '_polynomial', '_target_encoder', '_target_encoder_fast', 'add_dummy_feature', 'binarize', 'label_binarize', 'maxabs_scale', 'minmax_scale', 'normalize', 'power_transform', 'quantile_transform', 'robust_scale', 'scale']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "print(dir(preprocessing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LabelEncoder in module sklearn.preprocessing._label:\n",
      "\n",
      "class LabelEncoder(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  Encode target labels with value between 0 and n_classes-1.\n",
      " |\n",
      " |  This transformer should be used to encode target values, *i.e.* `y`, and\n",
      " |  not the input `X`.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <preprocessing_targets>`.\n",
      " |\n",
      " |  .. versionadded:: 0.12\n",
      " |\n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      Holds the label for each class.\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  OrdinalEncoder : Encode categorical features using an ordinal encoding\n",
      " |      scheme.\n",
      " |  OneHotEncoder : Encode categorical features as a one-hot numeric array.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  `LabelEncoder` can be used to normalize labels.\n",
      " |\n",
      " |  >>> from sklearn.preprocessing import LabelEncoder\n",
      " |  >>> le = LabelEncoder()\n",
      " |  >>> le.fit([1, 2, 2, 6])\n",
      " |  LabelEncoder()\n",
      " |  >>> le.classes_\n",
      " |  array([1, 2, 6])\n",
      " |  >>> le.transform([1, 1, 2, 6])\n",
      " |  array([0, 0, 1, 2]...)\n",
      " |  >>> le.inverse_transform([0, 0, 1, 2])\n",
      " |  array([1, 1, 2, 6])\n",
      " |\n",
      " |  It can also be used to transform non-numerical labels (as long as they are\n",
      " |  hashable and comparable) to numerical labels.\n",
      " |\n",
      " |  >>> le = LabelEncoder()\n",
      " |  >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
      " |  LabelEncoder()\n",
      " |  >>> list(le.classes_)\n",
      " |  ['amsterdam', 'paris', 'tokyo']\n",
      " |  >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
      " |  array([2, 2, 1]...)\n",
      " |  >>> list(le.inverse_transform([2, 2, 1]))\n",
      " |  ['tokyo', 'tokyo', 'paris']\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      LabelEncoder\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.utils._set_output._SetOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  fit(self, y)\n",
      " |      Fit label encoder.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |          Fitted label encoder.\n",
      " |\n",
      " |  fit_transform(self, y)\n",
      " |      Fit label encoder and return encoded labels.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Encoded labels.\n",
      " |\n",
      " |  inverse_transform(self, y)\n",
      " |      Transform labels back to original encoding.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Target values.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Original encoding.\n",
      " |\n",
      " |  transform(self, y)\n",
      " |      Transform labels to normalized encoding.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Labels as normalized encodings.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  set_output(self, *, transform=None)\n",
      " |      Set output container.\n",
      " |\n",
      " |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      " |      for an example on how to use the API.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      transform : {\"default\", \"pandas\"}, default=None\n",
      " |          Configure output of `transform` and `fit_transform`.\n",
      " |\n",
      " |          - `\"default\"`: Default output format of a transformer\n",
      " |          - `\"pandas\"`: DataFrame output\n",
      " |          - `\"polars\"`: Polars output\n",
      " |          - `None`: Transform configuration is unchanged\n",
      " |\n",
      " |          .. versionadded:: 1.4\n",
      " |              `\"polars\"` option was added.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  __sklearn_clone__(self)\n",
      " |\n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |\n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |\n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "print(help(preprocessing.LabelEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BootstrapMethod', 'CensoredData', 'ConstantInputWarning', 'Covariance', 'DegenerateDataWarning', 'FitError', 'MonteCarloMethod', 'NearConstantInputWarning', 'PermutationMethod', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_ansari_swilk_statistics', '_axis_nan_policy', '_biasedurn', '_binned_statistic', '_binomtest', '_boost', '_bws_test', '_censored_data', '_common', '_constants', '_continuous_distns', '_covariance', '_crosstab', '_discrete_distns', '_distn_infrastructure', '_distr_params', '_entropy', '_fit', '_hypotests', '_kde', '_ksstats', '_levy_stable', '_mannwhitneyu', '_morestats', '_mstats_basic', '_mstats_extras', '_multicomp', '_multivariate', '_mvn', '_odds_ratio', '_page_trend_test', '_qmc', '_qmc_cy', '_qmvnt', '_rcont', '_relative_risk', '_resampling', '_rvs_sampling', '_sampling', '_sensitivity_analysis', '_sobol', '_stats', '_stats_mstats_common', '_stats_py', '_stats_pythran', '_survival', '_tukeylambda_stats', '_unuran', '_variation', '_warnings_errors', '_wilcoxon', 'alexandergovern', 'alpha', 'anderson', 'anderson_ksamp', 'anglit', 'ansari', 'arcsine', 'argus', 'barnard_exact', 'bartlett', 'bayes_mvs', 'bernoulli', 'beta', 'betabinom', 'betanbinom', 'betaprime', 'biasedurn', 'binned_statistic', 'binned_statistic_2d', 'binned_statistic_dd', 'binom', 'binomtest', 'boltzmann', 'bootstrap', 'boschloo_exact', 'boxcox', 'boxcox_llf', 'boxcox_normmax', 'boxcox_normplot', 'bradford', 'brunnermunzel', 'burr', 'burr12', 'bws_test', 'cauchy', 'chi', 'chi2', 'chi2_contingency', 'chisquare', 'circmean', 'circstd', 'circvar', 'combine_pvalues', 'contingency', 'cosine', 'cramervonmises', 'cramervonmises_2samp', 'crystalball', 'cumfreq', 'describe', 'dgamma', 'differential_entropy', 'directional_stats', 'dirichlet', 'dirichlet_multinomial', 'distributions', 'dlaplace', 'dunnett', 'dweibull', 'ecdf', 'energy_distance', 'entropy', 'epps_singleton_2samp', 'erlang', 'expectile', 'expon', 'exponnorm', 'exponpow', 'exponweib', 'f', 'f_oneway', 'false_discovery_control', 'fatiguelife', 'find_repeats', 'fisher_exact', 'fisk', 'fit', 'fligner', 'foldcauchy', 'foldnorm', 'friedmanchisquare', 'gamma', 'gausshyper', 'gaussian_kde', 'genexpon', 'genextreme', 'gengamma', 'genhalflogistic', 'genhyperbolic', 'geninvgauss', 'genlogistic', 'gennorm', 'genpareto', 'geom', 'gibrat', 'gmean', 'gompertz', 'goodness_of_fit', 'gstd', 'gumbel_l', 'gumbel_r', 'gzscore', 'halfcauchy', 'halfgennorm', 'halflogistic', 'halfnorm', 'hmean', 'hypergeom', 'hypsecant', 'invgamma', 'invgauss', 'invweibull', 'invwishart', 'iqr', 'jarque_bera', 'jf_skew_t', 'johnsonsb', 'johnsonsu', 'kappa3', 'kappa4', 'kde', 'kendalltau', 'kruskal', 'ks_1samp', 'ks_2samp', 'ksone', 'kstat', 'kstatvar', 'kstest', 'kstwo', 'kstwobign', 'kurtosis', 'kurtosistest', 'laplace', 'laplace_asymmetric', 'levene', 'levy', 'levy_l', 'levy_stable', 'linregress', 'loggamma', 'logistic', 'loglaplace', 'lognorm', 'logrank', 'logser', 'loguniform', 'lomax', 'mannwhitneyu', 'matrix_normal', 'maxwell', 'median_abs_deviation', 'median_test', 'mielke', 'mode', 'moment', 'monte_carlo_test', 'mood', 'morestats', 'moyal', 'mstats', 'mstats_basic', 'mstats_extras', 'multinomial', 'multiscale_graphcorr', 'multivariate_hypergeom', 'multivariate_normal', 'multivariate_t', 'mvn', 'mvsdist', 'nakagami', 'nbinom', 'ncf', 'nchypergeom_fisher', 'nchypergeom_wallenius', 'nct', 'ncx2', 'nhypergeom', 'norm', 'normaltest', 'norminvgauss', 'obrientransform', 'ortho_group', 'page_trend_test', 'pareto', 'pearson3', 'pearsonr', 'percentileofscore', 'permutation_test', 'planck', 'pmean', 'pointbiserialr', 'poisson', 'poisson_means_test', 'power_divergence', 'powerlaw', 'powerlognorm', 'powernorm', 'ppcc_max', 'ppcc_plot', 'probplot', 'qmc', 'quantile_test', 'randint', 'random_correlation', 'random_table', 'rankdata', 'ranksums', 'rayleigh', 'rdist', 'recipinvgauss', 'reciprocal', 'rel_breitwigner', 'relfreq', 'rice', 'rv_continuous', 'rv_discrete', 'rv_histogram', 'rvs_ratio_uniforms', 'sampling', 'scoreatpercentile', 'sem', 'semicircular', 'shapiro', 'siegelslopes', 'sigmaclip', 'skellam', 'skew', 'skewcauchy', 'skewnorm', 'skewtest', 'sobol_indices', 'somersd', 'spearmanr', 'special_ortho_group', 'stats', 'studentized_range', 't', 'test', 'theilslopes', 'tiecorrect', 'tmax', 'tmean', 'tmin', 'trapezoid', 'trapz', 'triang', 'trim1', 'trim_mean', 'trimboth', 'truncexpon', 'truncnorm', 'truncpareto', 'truncweibull_min', 'tsem', 'tstd', 'ttest_1samp', 'ttest_ind', 'ttest_ind_from_stats', 'ttest_rel', 'tukey_hsd', 'tukeylambda', 'tvar', 'uniform', 'uniform_direction', 'unitary_group', 'variation', 'vonmises', 'vonmises_fisher', 'vonmises_line', 'wald', 'wasserstein_distance', 'wasserstein_distance_nd', 'weibull_max', 'weibull_min', 'weightedtau', 'wilcoxon', 'wishart', 'wrapcauchy', 'yeojohnson', 'yeojohnson_llf', 'yeojohnson_normmax', 'yeojohnson_normplot', 'yulesimon', 'zipf', 'zipfian', 'zmap', 'zscore']\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(dir(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ttest_rel in module scipy.stats._stats_py:\n",
      "\n",
      "ttest_rel(a, b, axis=0, nan_policy='propagate', alternative='two-sided', *, keepdims=False)\n",
      "    Calculate the t-test on TWO RELATED samples of scores, a and b.\n",
      "\n",
      "    This is a test for the null hypothesis that two related or\n",
      "    repeated samples have identical average (expected) values.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    a, b : array_like\n",
      "        The arrays must have the same shape.\n",
      "    axis : int or None, default: 0\n",
      "        If an int, the axis of the input along which to compute the statistic.\n",
      "        The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
      "        corresponding element of the output.\n",
      "        If ``None``, the input will be raveled before computing the statistic.\n",
      "    nan_policy : {'propagate', 'omit', 'raise'}\n",
      "        Defines how to handle input NaNs.\n",
      "\n",
      "        - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
      "          which the  statistic is computed, the corresponding entry of the output\n",
      "          will be NaN.\n",
      "        - ``omit``: NaNs will be omitted when performing the calculation.\n",
      "          If insufficient data remains in the axis slice along which the\n",
      "          statistic is computed, the corresponding entry of the output will be\n",
      "          NaN.\n",
      "        - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
      "    alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "        Defines the alternative hypothesis.\n",
      "        The following options are available (default is 'two-sided'):\n",
      "\n",
      "        * 'two-sided': the means of the distributions underlying the samples\n",
      "          are unequal.\n",
      "        * 'less': the mean of the distribution underlying the first sample\n",
      "          is less than the mean of the distribution underlying the second\n",
      "          sample.\n",
      "        * 'greater': the mean of the distribution underlying the first\n",
      "          sample is greater than the mean of the distribution underlying\n",
      "          the second sample.\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "    keepdims : bool, default: False\n",
      "        If this is set to True, the axes which are reduced are left\n",
      "        in the result as dimensions with size one. With this option,\n",
      "        the result will broadcast correctly against the input array.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    result : `~scipy.stats._result_classes.TtestResult`\n",
      "        An object with the following attributes:\n",
      "\n",
      "        statistic : float or array\n",
      "            The t-statistic.\n",
      "        pvalue : float or array\n",
      "            The p-value associated with the given alternative.\n",
      "        df : float or array\n",
      "            The number of degrees of freedom used in calculation of the\n",
      "            t-statistic; this is one less than the size of the sample\n",
      "            (``a.shape[axis]``).\n",
      "\n",
      "            .. versionadded:: 1.10.0\n",
      "\n",
      "        The object also has the following method:\n",
      "\n",
      "        confidence_interval(confidence_level=0.95)\n",
      "            Computes a confidence interval around the difference in\n",
      "            population means for the given confidence level.\n",
      "            The confidence interval is returned in a ``namedtuple`` with\n",
      "            fields `low` and `high`.\n",
      "\n",
      "            .. versionadded:: 1.10.0\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    Examples for use are scores of the same set of student in\n",
      "    different exams, or repeated sampling from the same units. The\n",
      "    test measures whether the average score differs significantly\n",
      "    across samples (e.g. exams). If we observe a large p-value, for\n",
      "    example greater than 0.05 or 0.1 then we cannot reject the null\n",
      "    hypothesis of identical average scores. If the p-value is smaller\n",
      "    than the threshold, e.g. 1%, 5% or 10%, then we reject the null\n",
      "    hypothesis of equal averages. Small p-values are associated with\n",
      "    large t-statistics.\n",
      "\n",
      "    The t-statistic is calculated as ``np.mean(a - b)/se``, where ``se`` is the\n",
      "    standard error. Therefore, the t-statistic will be positive when the sample\n",
      "    mean of ``a - b`` is greater than zero and negative when the sample mean of\n",
      "    ``a - b`` is less than zero.\n",
      "\n",
      "    Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
      "    code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
      "    this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
      "    rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
      "    arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
      "    masked array with ``mask=False``.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    https://en.wikipedia.org/wiki/T-test#Dependent_t-test_for_paired_samples\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from scipy import stats\n",
      "    >>> rng = np.random.default_rng()\n",
      "\n",
      "    >>> rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
      "    >>> rvs2 = (stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
      "    ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n",
      "    >>> stats.ttest_rel(rvs1, rvs2)\n",
      "    TtestResult(statistic=-0.4549717054410304, pvalue=0.6493274702088672, df=499)\n",
      "    >>> rvs3 = (stats.norm.rvs(loc=8, scale=10, size=500, random_state=rng)\n",
      "    ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n",
      "    >>> stats.ttest_rel(rvs1, rvs3)\n",
      "    TtestResult(statistic=-5.879467544540889, pvalue=7.540777129099917e-09, df=499)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(help(stats.ttest_rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BayesGaussMI', 'BinomialBayesMixedGLM', 'ConditionalLogit', 'ConditionalMNLogit', 'ConditionalPoisson', 'Factor', 'GEE', 'GLM', 'GLMGam', 'GLS', 'GLSAR', 'GeneralizedPoisson', 'HurdleCountModel', 'Logit', 'MANOVA', 'MI', 'MICE', 'MICEData', 'MNLogit', 'MixedLM', 'NegativeBinomial', 'NegativeBinomialP', 'NominalGEE', 'OLS', 'OrdinalGEE', 'PCA', 'PHReg', 'Poisson', 'PoissonBayesMixedGLM', 'ProbPlot', 'Probit', 'QuantReg', 'RLM', 'RecursiveLS', 'SurvfuncRight', 'TruncatedLFNegativeBinomialP', 'TruncatedLFPoisson', 'WLS', 'ZeroInflatedGeneralizedPoisson', 'ZeroInflatedNegativeBinomialP', 'ZeroInflatedPoisson', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '__version__', '__version_info__', 'add_constant', 'categorical', 'cov_struct', 'datasets', 'distributions', 'duration', 'emplike', 'families', 'formula', 'gam', 'genmod', 'graphics', 'iolib', 'load', 'load_pickle', 'multivariate', 'nonparametric', 'qqline', 'qqplot', 'qqplot_2samples', 'regression', 'robust', 'show_versions', 'stats', 'test', 'tools', 'tsa', 'webdoc']\n",
      "['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'conditional_logit', 'conditional_mnlogit', 'conditional_poisson', 'gee', 'glm', 'glmgam', 'gls', 'glsar', 'logit', 'mixedlm', 'mnlogit', 'negativebinomial', 'nominal_gee', 'ols', 'ordinal_gee', 'phreg', 'poisson', 'probit', 'quantreg', 'rlm', 'wls']\n",
      "Help on method from_formula in module statsmodels.base.model:\n",
      "\n",
      "from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) class method of statsmodels.regression.linear_model.OLS\n",
      "    Create a Model from a formula and dataframe.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    formula : str or generic Formula object\n",
      "        The formula specifying the model.\n",
      "    data : array_like\n",
      "        The data for the model. See Notes.\n",
      "    subset : array_like\n",
      "        An array-like object of booleans, integers, or index values that\n",
      "        indicate the subset of df to use in the model. Assumes df is a\n",
      "        `pandas.DataFrame`.\n",
      "    drop_cols : array_like\n",
      "        Columns to drop from the design matrix.  Cannot be used to\n",
      "        drop terms involving categoricals.\n",
      "    *args\n",
      "        Additional positional argument that are passed to the model.\n",
      "    **kwargs\n",
      "        These are passed to the model with one exception. The\n",
      "        ``eval_env`` keyword is passed to patsy. It can be either a\n",
      "        :class:`patsy:patsy.EvalEnvironment` object or an integer\n",
      "        indicating the depth of the namespace to use. For example, the\n",
      "        default ``eval_env=0`` uses the calling namespace. If you wish\n",
      "        to use a \"clean\" environment set ``eval_env=-1``.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    model\n",
      "        The model instance.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    data must define __getitem__ with the keys in the formula terms\n",
      "    args and kwargs are passed on to the model instantiation. E.g.,\n",
      "    a numpy structured or rec array, a dictionary, or a pandas DataFrame.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "print(dir(sm))\n",
    "print(dir(sm.formula))\n",
    "print(help(sm.formula.ols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__init__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '__version_info__', '__version_tuple__', '_version', 'api', 'base', 'compat', 'datasets', 'debug_warnings', 'discrete', 'distributions', 'duration', 'emplike', 'formula', 'gam', 'genmod', 'graphics', 'imputation', 'iolib', 'monkey_patch_cat_dtype', 'multivariate', 'nonparametric', 'regression', 'robust', 'sandbox', 'stats', 'test', 'tools', 'tsa']\n",
      "['PytestTester', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_adnorm', '_inference_tools', '_knockoff', '_lilliefors', '_lilliefors_critical_values', 'anova', 'api', 'base', 'contingency_tables', 'contrast', 'correlation_tools', 'descriptivestats', 'diagnostic', 'gof', 'inter_rater', 'mediation', 'meta_analysis', 'moment_helpers', 'multicomp', 'multitest', 'multivariate', 'oaxaca', 'oneway', 'power', 'proportion', 'rates', 'robust_compare', 'sandwich_covariance', 'stattools', 'tabledist', 'test', 'weightstats']\n",
      "['AnovaRM', 'AnovaResults', 'DataFrame', 'Index', 'OLS', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_get_covariance', '_has_intercept', '_intercept_idx', '_not_slice', '_remove_intercept_patsy', '_ssr_reduced_model', 'anova1_lm_single', 'anova2_lm_single', 'anova3_lm_single', 'anova_lm', 'anova_single', 'lrange', 'np', 'patsy', 'pd', 'stats', 'summary2']\n",
      "Help on function anova_lm in module statsmodels.stats.anova:\n",
      "\n",
      "anova_lm(*args, **kwargs)\n",
      "    Anova table for one or more fitted linear models.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    args : fitted linear model results instance\n",
      "        One or more fitted linear models\n",
      "    scale : float\n",
      "        Estimate of variance, If None, will be estimated from the largest\n",
      "        model. Default is None.\n",
      "    test : str {\"F\", \"Chisq\", \"Cp\"} or None\n",
      "        Test statistics to provide. Default is \"F\".\n",
      "    typ : str or int {\"I\",\"II\",\"III\"} or {1,2,3}\n",
      "        The type of Anova test to perform. See notes.\n",
      "    robust : {None, \"hc0\", \"hc1\", \"hc2\", \"hc3\"}\n",
      "        Use heteroscedasticity-corrected coefficient covariance matrix.\n",
      "        If robust covariance is desired, it is recommended to use `hc3`.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    anova : DataFrame\n",
      "        When args is a single model, return is DataFrame with columns:\n",
      "\n",
      "        sum_sq : float64\n",
      "            Sum of squares for model terms.\n",
      "        df : float64\n",
      "            Degrees of freedom for model terms.\n",
      "        F : float64\n",
      "            F statistic value for significance of adding model terms.\n",
      "        PR(>F) : float64\n",
      "            P-value for significance of adding model terms.\n",
      "\n",
      "        When args is multiple models, return is DataFrame with columns:\n",
      "\n",
      "        df_resid : float64\n",
      "            Degrees of freedom of residuals in models.\n",
      "        ssr : float64\n",
      "            Sum of squares of residuals in models.\n",
      "        df_diff : float64\n",
      "            Degrees of freedom difference from previous model in args\n",
      "        ss_dff : float64\n",
      "            Difference in ssr from previous model in args\n",
      "        F : float64\n",
      "            F statistic comparing to previous model in args\n",
      "        PR(>F): float64\n",
      "            P-value for significance comparing to previous model in args\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    Model statistics are given in the order of args. Models must have been fit\n",
      "    using the formula api.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    model_results.compare_f_test, model_results.compare_lm_test\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> import statsmodels.api as sm\n",
      "    >>> from statsmodels.formula.api import ols\n",
      "    >>> moore = sm.datasets.get_rdataset(\"Moore\", \"carData\", cache=True) # load\n",
      "    >>> data = moore.data\n",
      "    >>> data = data.rename(columns={\"partner.status\" :\n",
      "    ...                             \"partner_status\"}) # make name pythonic\n",
      "    >>> moore_lm = ols('conformity ~ C(fcategory, Sum)*C(partner_status, Sum)',\n",
      "    ...                 data=data).fit()\n",
      "    >>> table = sm.stats.anova_lm(moore_lm, typ=2) # Type 2 Anova DataFrame\n",
      "    >>> print(table)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels\n",
    "\n",
    "print(dir(statsmodels))\n",
    "print(dir(statsmodels.stats))\n",
    "print(dir(statsmodels.stats.anova))\n",
    "print(help(statsmodels.stats.anova.anova_lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
